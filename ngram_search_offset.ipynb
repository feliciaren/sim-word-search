{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/llluckygirl/envs/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from random import choice\n",
    "from itertools import groupby\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KB(object):\n",
    "    def __init__(self,kb_directory):\n",
    "        print(\"start loading kb_data...\")\n",
    "        self.kb_directory = kb_directory\n",
    "        self.id2kb,self.types,self.predicate = self.get_id2kb()\n",
    "        self.kb2id = self.get_kb2id()\n",
    "        self.kb = list(self.kb2id.keys())\n",
    "        self.id = list(self.id2kb.keys())\n",
    "        print(\"KB DATA INFORMATION\")\n",
    "        print(\"TOKEN SIZE:{}\".format(self.get_token_size()))\n",
    "        print(\"ID SIZE:{}\".format(len(self)))\n",
    "        print(\"TYPE SIZE:{}\".format(len(self.types)))\n",
    "        print(\"PREDICATE SIZE:{}\".format(len(self.predicate)))\n",
    "    def get_id2kb(self):\n",
    "        print(\"construct id2kb dict...\")\n",
    "        id2kb = {}\n",
    "        kbtype = set()\n",
    "        predicate = set()\n",
    "        multi_type = []\n",
    "        with open(self.kb_directory) as f:\n",
    "            for l in tqdm(f):\n",
    "                tmp = json.loads(l)\n",
    "                subject_id = tmp['subject_id']\n",
    "                subject_alias = list(set([tmp['subject']] + tmp.get('alias', [])))\n",
    "                subject_alias = [alias.lower() for alias in subject_alias]\n",
    "                subject_type = [i.lower() for i in tmp['type']]\n",
    "                kbtype.update(subject_type)\n",
    "                try:\n",
    "                    assert(len(tmp['type'])==1)\n",
    "                except AssertionError:\n",
    "                    multi_type.append(tmp['type'])\n",
    "                subject_data = {}\n",
    "                for i in tmp['data']:\n",
    "                    predicate.add(i['predicate'].lower())\n",
    "                    subject_data[i['predicate'].lower()] = i['object'].lower()\n",
    "                if subject_data:\n",
    "                    id2kb[subject_id] = {'alias': subject_alias, 'data': subject_data,'type':subject_type}\n",
    "#         print(multi_type)\n",
    "        return id2kb,kbtype,predicate\n",
    "    def get_kb2id(self):\n",
    "        print(\"construct kb2id dict...\")\n",
    "        kb2id = {}\n",
    "        for i,j in self.id2kb.items():\n",
    "            for k in j['alias']:\n",
    "                if k not in kb2id:\n",
    "                    kb2id[k] = []\n",
    "                kb2id[k].append(i)\n",
    "        return kb2id\n",
    "    def __len__(self):\n",
    "        return len(self.id2kb)\n",
    "    def get_token_size(self):\n",
    "        return len(self.kb)\n",
    "#     def save(self):\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1532it [00:00, 15317.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start loading kb_data...\n",
      "construct id2kb dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "399252it [00:20, 19298.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "construct kb2id dict...\n",
      "KB DATA INFORMATION\n",
      "TOKEN SIZE:303375\n",
      "ID SIZE:399233\n",
      "TYPE SIZE:51\n",
      "PREDICATE SIZE:41841\n"
     ]
    }
   ],
   "source": [
    "kb_data = KB('./ccks2019_el/kb_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ngram_search(object):\n",
    "    def __init__(self,data,kb,ngram = 2,similarity = 0.5):\n",
    "        self.n = ngram\n",
    "        self.similarity = similarity\n",
    "        self.data = data\n",
    "        self.kb = kb\n",
    "        self.cut_data,self.offset = self.cut_words()\n",
    "        self.ts = TopSim(self.kb)\n",
    "        self.candidates = self.get_candidates(self.similarity)\n",
    "        self.cand_name,self.cand_off = self.get_candidates_name()\n",
    "        self.cand_with_off = self.get_cand_with_off(self.similarity)\n",
    "    def cut_words(self):\n",
    "        print('starting build ngram list')\n",
    "        print('ngram',self.n)\n",
    "        result = []\n",
    "        offset = []\n",
    "        for d in tqdm(self.data):\n",
    "#             print(d)\n",
    "#             print(' '.join(jieba.cut(d)))\n",
    "            tmp = list(jieba.cut(d))\n",
    "            n = len(tmp)\n",
    "            tmp_off = [0]\n",
    "#             tmp_off = [len(''.join(tmp[:i])) for i in range(len(tmp))]\n",
    "            for i in range(len(tmp)-1):\n",
    "                tmp_off.append(tmp_off[-1]+len(tmp[i]))\n",
    "            for j in range(2,self.n+1):\n",
    "                for i in range(j-1,n):\n",
    "                    tmp.append(''.join(tmp[i-j+1:i+1]))\n",
    "                    tmp_off.append(tmp_off[i-j+1])\n",
    "#                     tmp_off.append(''.join(tmp[:i-n+1]))\n",
    "            result.append(tmp)\n",
    "            offset.append(tmp_off)\n",
    "        return result,offset\n",
    "                                             \n",
    "    def get_candidates(self,similarity = 0.5):\n",
    "        self.similarity = similarity\n",
    "        print('starting build candidates list')\n",
    "        print('similarity:',self.similarity)\n",
    "        candidates = []\n",
    "        for dt in tqdm(self.cut_data):\n",
    "            ts_result = []\n",
    "            for i in dt:\n",
    "                tmp = ts.search(i)\n",
    "                if tmp and tmp[0][0] > self.similarity:\n",
    "                    ts_result.append(tmp)\n",
    "                else:\n",
    "                    ts_result.append([])\n",
    "            candidates.append(ts_result)\n",
    "        return candidates\n",
    "    \n",
    "    def get_candidates_name(self):\n",
    "        print('starting get candidates name and offset')\n",
    "        cand_name = []\n",
    "        cand_offset = []\n",
    "        for i in tqdm(range(len(self.candidates))):\n",
    "            cand = []\n",
    "            off = []\n",
    "            for j in range(len(self.candidates[i])):\n",
    "                if self.candidates[i][j]:\n",
    "#                     print(self.candidates[i][j])\n",
    "#                     print(self.candidates[i][j][0][1][0])\n",
    "#                     print(self.kb[self.candidates[i][j][0][1][0]])\n",
    "                    cand.append(self.kb[self.candidates[i][j][0][1][0]])\n",
    "                    off.append(self.offset[i][j])\n",
    "            cand_name.append(cand)\n",
    "            cand_offset.append(off)\n",
    "        return cand_name,cand_offset\n",
    "    \n",
    "    def get_cand_with_off(self):\n",
    "        print('starting get candidates with offset')\n",
    "        cand_with_off = []\n",
    "        for i in tqdm(range(len(self.cand_name))):\n",
    "            c_o = {}\n",
    "            for j in range(len(self.cand_name[i]))\n",
    "                offset = self.cand_offset[i][j]\n",
    "                cand = self.cand_name = \n",
    "                if offset not in c_o:\n",
    "                    c_o[str(offset)] = []\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
