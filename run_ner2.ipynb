{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import argparse\n",
    "import random\n",
    "import copy\n",
    "import torch\n",
    "import gc\n",
    "# import cPickle as pickle\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from random import choice\n",
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ner import *\n",
    "from ner_utils.metric import get_ner_fmeasure\n",
    "from ner_utils.data import Data\n",
    "from KB import KB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_directory):\n",
    "    with open(data_directory) as f:\n",
    "        train_data,dev_data,test_data = json.load(f)\n",
    "#         train_data = byteify(train_data)\n",
    "#         dev_data = byteify(dev_data)\n",
    "#         test_data = byteify(test_data)\n",
    "    print('traindata size:',len(train_data))\n",
    "    print('devdata size:',len(dev_data))\n",
    "    print('testdata size',len(test_data))\n",
    "    return train_data,dev_data,test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ner_data(data):\n",
    "    char = []\n",
    "    seg = []\n",
    "    label = []\n",
    "    ids = []\n",
    "    kb_ids = []\n",
    "    for i in tqdm(range(len(data))):\n",
    "        tmp_text = list(data[i]['text'])\n",
    "        tmp_label = ['O']*len(tmp_text)\n",
    "        tmp_mention = data[i]['mention_data']\n",
    "        kb_id = []\n",
    "        for j in range(len(tmp_mention)):\n",
    "            m,o,_id = tmp_mention[j]\n",
    "            kb_id.append(_id)\n",
    "            if len(m) > 1:\n",
    "                tmp_label[o] = 'B'\n",
    "                tmp_label[o+len(m)-1] = 'E'\n",
    "                for j in range(o+1,o+len(m)-1,1):\n",
    "                    tmp_label[j] = 'M'\n",
    "            else:\n",
    "                tmp_label[o] = 'S'\n",
    "        seg.append(list(jieba.cut(data[i]['text'])))\n",
    "        char.append(tmp_text)\n",
    "        label.append(tmp_label)\n",
    "        ids.append(data[i]['text_id'])\n",
    "        kb_ids.append(kb_id)\n",
    "    return char,seg,label,ids,kb_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_initialization(data, kb, train_file, dev_file, test_file):\n",
    "    data.build_alphabet(train_file[0],train_file[1])\n",
    "    data.build_alphabet(dev_file[0],dev_file[1])\n",
    "    data.build_alphabet(test_file[0],test_file[1])\n",
    "    data.build_kb(kb)\n",
    "    data.build_all_alphabet(train_file[0],train_file[1])\n",
    "    data.build_all_alphabet(train_file[0],train_file[1])\n",
    "    data.build_all_alphabet(train_file[0],train_file[1])\n",
    "    data.fix_alphabet()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def train(data, save_model_dir, seg=True):\n",
    "    print (\"Training model...\")\n",
    "    data.show_data_summary()\n",
    "    save_data_name = save_model_dir +\".dset\"\n",
    "    save_data_setting(data, save_data_name)\n",
    "#     ????????????\n",
    "#     model = SeqModel(data)\n",
    "    print \"finished built model.\"\n",
    "    loss_function = nn.NLLLoss()\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = optim.SGD(parameters, lr=data.HP_lr, momentum=data.HP_momentum)\n",
    "    best_dev = -1\n",
    "    data.HP_iteration = 100\n",
    "    ## start training\n",
    "    for idx in range(data.HP_iteration):\n",
    "        epoch_start = time.time()\n",
    "        temp_start = epoch_start\n",
    "        print(\"Epoch: %s/%s\" %(idx,data.HP_iteration))\n",
    "        optimizer = lr_decay(optimizer, idx, data.HP_lr_decay, data.HP_lr)\n",
    "        instance_count = 0\n",
    "        sample_id = 0\n",
    "        sample_loss = 0\n",
    "        batch_loss = 0\n",
    "        total_loss = 0\n",
    "        right_token = 0\n",
    "        whole_token = 0\n",
    "        random.shuffle(data.train_Ids)\n",
    "        ## set model in train model\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        batch_size = 1 ## current only support batch size = 1 to compulate and accumulate to data.HP_batch_size update weights\n",
    "        batch_id = 0\n",
    "        train_num = len(data.train_Ids)\n",
    "        total_batch = train_num//batch_size+1\n",
    "        for batch_id in range(total_batch):\n",
    "            start = batch_id*batch_size\n",
    "            end = (batch_id+1)*batch_size \n",
    "            if end >train_num:\n",
    "                end = train_num\n",
    "            instance = data.train_Ids[start:end]\n",
    "            if not instance:\n",
    "                continue\n",
    "            gaz_list,  batch_word, batch_biword, batch_wordlen, batch_wordrecover, batch_char, batch_charlen, batch_charrecover, batch_label, mask  = batchify_with_label(instance, data.HP_gpu)\n",
    "            # print \"gaz_list:\",gaz_list\n",
    "            # exit(0)\n",
    "            instance_count += 1\n",
    "            loss, tag_seq = model.neg_log_likelihood_loss(gaz_list, batch_word, batch_biword, batch_wordlen, batch_char, batch_charlen, batch_charrecover, batch_label, mask)\n",
    "            right, whole = predict_check(tag_seq, batch_label, mask)\n",
    "            right_token += right\n",
    "            whole_token += whole\n",
    "            sample_loss += loss.data[0]\n",
    "            total_loss += loss.data[0]\n",
    "            batch_loss += loss\n",
    "\n",
    "            if end%500 == 0:\n",
    "                temp_time = time.time()\n",
    "                temp_cost = temp_time - temp_start\n",
    "                temp_start = temp_time\n",
    "                print(\"     Instance: %s; Time: %.2fs; loss: %.4f; acc: %s/%s=%.4f\"%(end, temp_cost, sample_loss, right_token, whole_token,(right_token+0.)/whole_token))\n",
    "                sys.stdout.flush()\n",
    "                sample_loss = 0\n",
    "            if end%data.HP_batch_size == 0:\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "                model.zero_grad()\n",
    "                batch_loss = 0\n",
    "        temp_time = time.time()\n",
    "        temp_cost = temp_time - temp_start\n",
    "        print(\"     Instance: %s; Time: %.2fs; loss: %.4f; acc: %s/%s=%.4f\"%(end, temp_cost, sample_loss, right_token, whole_token,(right_token+0.)/whole_token))       \n",
    "        epoch_finish = time.time()\n",
    "        epoch_cost = epoch_finish - epoch_start\n",
    "        print(\"Epoch: %s training finished. Time: %.2fs, speed: %.2fst/s,  total loss: %s\"%(idx, epoch_cost, train_num/epoch_cost, total_loss))\n",
    "        # exit(0)\n",
    "        # continue\n",
    "        speed, acc, p, r, f, _ = evaluate(data, model, \"dev\")\n",
    "        dev_finish = time.time()\n",
    "        dev_cost = dev_finish - epoch_finish\n",
    "\n",
    "        if seg:\n",
    "            current_score = f\n",
    "            print(\"Dev: time: %.2fs, speed: %.2fst/s; acc: %.4f, p: %.4f, r: %.4f, f: %.4f\"%(dev_cost, speed, acc, p, r, f))\n",
    "        else:\n",
    "            current_score = acc\n",
    "            print(\"Dev: time: %.2fs speed: %.2fst/s; acc: %.4f\"%(dev_cost, speed, acc))\n",
    "\n",
    "        if current_score > best_dev:\n",
    "            if seg:\n",
    "                print \"Exceed previous best f score:\", best_dev\n",
    "            else:\n",
    "                print \"Exceed previous best acc score:\", best_dev\n",
    "            model_name = save_model_dir +'.'+ str(idx) + \".model\"\n",
    "            torch.save(model.state_dict(), model_name)\n",
    "            best_dev = current_score \n",
    "        # ## decode test\n",
    "        speed, acc, p, r, f, _ = evaluate(data, model, \"test\")\n",
    "        test_finish = time.time()\n",
    "        test_cost = test_finish - dev_finish\n",
    "        if seg:\n",
    "            print(\"Test: time: %.2fs, speed: %.2fst/s; acc: %.4f, p: %.4f, r: %.4f, f: %.4f\"%(test_cost, speed, acc, p, r, f))\n",
    "        else:\n",
    "            print(\"Test: time: %.2fs, speed: %.2fst/s; acc: %.4f\"%(test_cost, speed, acc))\n",
    "        gc.collect() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_num = 100\n",
    "random.seed(seed_num)\n",
    "torch.manual_seed(seed_num)\n",
    "np.random.seed(seed_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Tuning with bi-directional LSTM-CRF')\n",
    "parser.add_argument('--embedding',  help='Embedding for words', default='None')\n",
    "parser.add_argument('--status', choices=['train', 'test', 'decode'], help='update algorithm', default='train')\n",
    "parser.add_argument('--savemodel', default=\"data/model/saved_model.lstmcrf.\")\n",
    "parser.add_argument('--savedset', help='Dir of saved data setting', default=\"data/save.dset\")\n",
    "parser.add_argument('--data',default=\"data/all_data.json\")\n",
    "# parser.add_argument('--train', default=\"data/conll03/train.bmes\") \n",
    "# parser.add_argument('--dev', default=\"data/conll03/dev.bmes\" )  \n",
    "# parser.add_argument('--test', default=\"data/conll03/test.bmes\") \n",
    "parser.add_argument('--seg', default=\"True\") \n",
    "parser.add_argument('--extendalphabet', default=\"True\") \n",
    "# parser.add_argument('--raw') \n",
    "parser.add_argument('--loadmodel')\n",
    "parser.add_argument('--output') \n",
    "# ?????????????????????????????????\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = args.data\n",
    "model_dir = args.loadmodel\n",
    "dset_dir = args.savedset\n",
    "kb_file = None\n",
    "output_file = args.output\n",
    "if args.seg.lower() == \"true\":\n",
    "    seg = True \n",
    "else:\n",
    "    seg = False\n",
    "status = args.status.lower()\n",
    "save_model_dir = args.savemodel\n",
    "gpu = torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_emb = None\n",
    "bichar_emb = None\n",
    "# kb_emb = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CuDNN: True\n",
      "GPU available: True\n",
      "Status: train\n",
      "Seg:  True\n",
      "Data dir: data/all_data.json\n",
      "Char emb: None\n",
      "Bichar emb: None\n",
      "KB file: None\n",
      "Model saved to: data/model/saved_model.lstmcrf.\n"
     ]
    }
   ],
   "source": [
    "print (\"CuDNN:\", torch.backends.cudnn.enabled)\n",
    "# gpu = False\n",
    "print (\"GPU available:\", gpu)\n",
    "print (\"Status:\", status)\n",
    "print (\"Seg: \", seg)\n",
    "# print (\"Train file:\", train_file)\n",
    "# print (\"Dev file:\", dev_file)\n",
    "# print (\"Test file:\", test_file)\n",
    "print (\"Data dir:\", data_dir)\n",
    "# print (\"Raw file:\", raw_file)\n",
    "print (\"Char emb:\", char_emb)\n",
    "print (\"Bichar emb:\", bichar_emb)\n",
    "print (\"KB file:\",kb_file)\n",
    "if status == 'train':\n",
    "    print (\"Model saved to:\", save_model_dir)\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2330it [00:00, 23283.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traindata size: 84262\n",
      "devdata size: 851\n",
      "testdata size 852\n",
      "start loading kb_data...\n",
      "construct id2kb dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "399252it [00:18, 21966.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "construct kb2id dict...\n",
      "KB DATA INFORMATION\n",
      "TOKEN SIZE:303375\n",
      "ID SIZE:399233\n",
      "TYPE SIZE:51\n",
      "PREDICATE SIZE:41841\n"
     ]
    }
   ],
   "source": [
    "# train_file = args.train\n",
    "# dev_file = args.dev\n",
    "# test_file = args.test\n",
    "# raw_file = args.raw\n",
    "train_data,dev_data,test_data = load_data('./data/all_data.json')\n",
    "kb_data = KB('./ccks2019_el/kb_data')\n",
    "# ??????????????????\n",
    "kb = kb_data.kb[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/84262 [00:00<?, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 1.077 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "100%|██████████| 84262/84262 [00:18<00:00, 4574.64it/s]\n",
      "100%|██████████| 851/851 [00:00<00:00, 4904.58it/s]\n",
      "100%|██████████| 852/852 [00:00<00:00, 4665.20it/s]\n"
     ]
    }
   ],
   "source": [
    "train_char,train_seg,train_label,train_ids,train_kb_ids  = generate_ner_data(train_data)\n",
    "dev_char,dev_seg,dev_label,dev_ids,dev_kb_ids  = generate_ner_data(dev_data)\n",
    "test_char,test_seg,test_label,test_ids,test_kb_ids  = generate_ner_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KB total size: 100\n",
      "gaz alphabet size: 2\n",
      "kb alphabet size: 70\n",
      "gaz alphabet size: 2\n",
      "kb alphabet size: 70\n",
      "gaz alphabet size: 2\n",
      "kb alphabet size: 70\n",
      "build word pretrain emb...\n",
      "Embedding:\n",
      "     pretrain word:0, prefect match:0, case_match:0, oov:6253, oov%:0.9998401023345059\n",
      "build biword pretrain emb...\n",
      "Embedding:\n",
      "     pretrain word:0, prefect match:0, case_match:0, oov:302399, oov%:0.9999966931216931\n",
      "build gaz pretrain emb...\n",
      "Embedding:\n",
      "     pretrain word:0, prefect match:0, case_match:0, oov:69, oov%:0.9857142857142858\n"
     ]
    }
   ],
   "source": [
    "if status == 'train':\n",
    "        data = Data()\n",
    "        data.HP_gpu = gpu\n",
    "        data.HP_use_char = False\n",
    "        data.HP_batch_size = 1\n",
    "        data.use_bigram = False\n",
    "        data.gaz_dropout = 0.5\n",
    "        data.norm_gaz_emb = False\n",
    "        data.HP_fix_gaz_emb = False\n",
    "        data_initialization(data, kb, [train_char,train_label], [dev_char,dev_label], [test_char,test_label])\n",
    "        data.generate_instance_with_kb(train_char,train_label,'train')\n",
    "        data.generate_instance_with_kb(dev_char,dev_label,'dev')\n",
    "        data.generate_instance_with_kb(test_char,test_label,'test')\n",
    "        data.build_word_pretrain_emb(char_emb)\n",
    "        data.build_biword_pretrain_emb(bichar_emb)\n",
    "        data.build_kb_pretrain_emb(kb_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.1.post2\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "print(t.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
