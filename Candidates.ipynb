{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一步先写一个快速比较字符串，召回的类，使得召回率尽量达到百分之95左右，精度百分之30左右最好\n",
    "字符串距离函数采取jaccard 计算快\n",
    "下一步可增加编辑距离、cosine距离 来看一下速度快不快\n",
    "因为需要召回的速度快，召回的召回率高，因此需要损失掉较高的精度，下一步实体消歧义处理精度的问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from random import choice\n",
    "from itertools import groupby\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsim import TopSim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KB(object):\n",
    "    def __init__(self,kb_directory):\n",
    "        print(\"start loading kb_data...\")\n",
    "        self.kb_directory = kb_directory\n",
    "        self.id2kb,self.types,self.predicate = self.get_id2kb()\n",
    "        self.kb2id = self.get_kb2id()\n",
    "        self.kb = list(self.kb2id.keys())\n",
    "        self.id = list(self.id2kb.keys())\n",
    "        self.print_info()\n",
    "\n",
    "    def print_info(self):\n",
    "        print(\"KB DATA INFORMATION\")\n",
    "        print(\"TOKEN SIZE:{}\".format(self.get_token_size()))\n",
    "        print(\"ID SIZE:{}\".format(len(self)))\n",
    "        print(\"TYPE SIZE:{}\".format(len(self.types)))\n",
    "        print(\"PREDICATE SIZE:{}\".format(len(self.predicate)))\n",
    "        \n",
    "    def get_id2kb(self):\n",
    "        print(\"construct id2kb dict...\")\n",
    "        id2kb = {}\n",
    "        kbtype = set()\n",
    "        predicate = set()\n",
    "        multi_type = []\n",
    "        with open(self.kb_directory) as f:\n",
    "            for l in tqdm(f):\n",
    "                tmp = json.loads(l)\n",
    "                subject_id = tmp['subject_id']\n",
    "                subject_alias = list(set([tmp['subject']] + tmp.get('alias', [])))\n",
    "                subject_alias = [alias.lower() for alias in subject_alias]\n",
    "                subject_type = [i.lower() for i in tmp['type']]\n",
    "                kbtype.update(subject_type)\n",
    "                try:\n",
    "                    assert(len(tmp['type'])==1)\n",
    "                except AssertionError:\n",
    "                    multi_type.append(tmp['type'])\n",
    "                subject_data = {}\n",
    "                for i in tmp['data']:\n",
    "                    predicate.add(i['predicate'].lower())\n",
    "                    subject_data[i['predicate'].lower()] = i['object'].lower()\n",
    "                if subject_data:\n",
    "                    id2kb[subject_id] = {'alias': subject_alias, 'data': subject_data,'type':subject_type}\n",
    "#         print(multi_type)\n",
    "        return id2kb,kbtype,predicate\n",
    "\n",
    "    def get_kb2id(self):\n",
    "        print(\"construct kb2id dict...\")\n",
    "        kb2id = {}\n",
    "        for i,j in self.id2kb.items():\n",
    "            for k in j['alias']:\n",
    "                if k not in kb2id:\n",
    "                    kb2id[k] = []\n",
    "                kb2id[k].append(i)\n",
    "        return kb2id\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.id2kb)\n",
    "    \n",
    "    def get_token_size(self):\n",
    "        return len(self.kb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1959it [00:00, 19588.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start loading kb_data...\n",
      "construct id2kb dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "399252it [00:19, 20730.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "construct kb2id dict...\n",
      "KB DATA INFORMATION\n",
      "TOKEN SIZE:303375\n",
      "ID SIZE:399233\n",
      "TYPE SIZE:51\n",
      "PREDICATE SIZE:41841\n"
     ]
    }
   ],
   "source": [
    "kb_data = KB('./ccks2019_el/kb_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_directory):\n",
    "    with open(data_directory) as f:\n",
    "        train_data,dev_data,test_data = json.load(f)\n",
    "    print('traindata size:',len(train_data))\n",
    "    print('devdata size:',len(dev_data))\n",
    "    print('testdata size',len(test_data))\n",
    "    return train_data,dev_data,test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data,dev_data,test_data = load_data('./data/all_data.json')\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "class ngram_search(object):\n",
    "    \n",
    "    def __init__(self,data,kb,ngram = 4,similarity = 0.6,k=1,e=0.7,simFunc=\"tversky\"): \n",
    "        self.n = ngram\n",
    "        self.similarity = similarity\n",
    "        self.data = data\n",
    "        self.kb = kb\n",
    "        self.k = k\n",
    "        self.e = e\n",
    "        self.simFunc = simFunc\n",
    "        self.cut_data,self.offset = self.cut_words()\n",
    "        self.ts = TopSim(self.kb)\n",
    "        self.candidates = self.get_candidates()\n",
    "        self.cand_name,self.cand_off,self.cand_with_off = self.get_candidates_name()\n",
    "        \n",
    "    def cut_words(self):\n",
    "        print('starting build ngram list')\n",
    "        print('ngram',self.n)\n",
    "        result = []\n",
    "        offset = []\n",
    "        for d in tqdm(self.data):\n",
    "#             print(d)\n",
    "#             print(' '.join(jieba.cut(d)))\n",
    "            tmp = list(jieba.cut(d))\n",
    "            n = len(tmp)\n",
    "            tmp_off = []\n",
    "            tmp_off.append((0,len(tmp[0])))\n",
    "#             tmp_off = [len(''.join(tmp[:i])) for i in range(len(tmp))]\n",
    "            for i in range(1,len(tmp)):\n",
    "                tmp_off.append((tmp_off[-1][1],tmp_off[-1][1]+len(tmp[i])))\n",
    "            for j in range(2,self.n+1):\n",
    "                for i in range(j-1,n):\n",
    "                    tmp.append(''.join(tmp[i-j+1:i+1]))\n",
    "                    tmp_off.append((tmp_off[i-j+1][0],tmp_off[i][1]))\n",
    "#                     tmp_off.append(''.join(tmp[:i-n+1]))\n",
    "            result.append(tmp)\n",
    "            offset.append(tmp_off)\n",
    "#         print(result[0])\n",
    "#         print(offset[0])\n",
    "        return result,offset\n",
    "    \n",
    "    def get_candidates(self):\n",
    "#         self.similarity = similarity\n",
    "        print('starting build candidates list')\n",
    "        print('similarity:',self.similarity)\n",
    "        candidates = []\n",
    "        for dt in tqdm(self.cut_data):\n",
    "            ts_result = []\n",
    "            for i in dt:\n",
    "                tmp = self.ts.search(i,k = self.k,e = self.e,worstSim = self.similarity,simFunc=self.simFunc)\n",
    "                if tmp:\n",
    "#                 if tmp and tmp[0][0] > self.similarity:\n",
    "                    ts_result.append(tmp)\n",
    "                else:\n",
    "                    ts_result.append([])\n",
    "            candidates.append(ts_result)\n",
    "        return candidates\n",
    "                                                     \n",
    "    def get_candidates_name(self):\n",
    "        print('starting get candidates name and offset')\n",
    "        cand_name = []\n",
    "        cand_offset = []\n",
    "        cand_with_off = []\n",
    "        cand_with_offend = []\n",
    "        for i in tqdm(range(len(self.candidates))):\n",
    "            cand = []\n",
    "            off = []\n",
    "            c_o_s = {}\n",
    "            c_o_e = {}\n",
    "            for j in range(len(self.candidates[i])):    \n",
    "                if self.candidates[i][j]:\n",
    "#                     print(self.candidates[i][j])\n",
    "#                     print(self.candidates[i][j][0][1][0])\n",
    "#                     print(self.kb[self.candidates[i][j][0][1][0]])\n",
    "                    token = self.kb[self.candidates[i][j][0][1][0]]\n",
    "                    token_off = self.offset[i][j]\n",
    "#                     print(token_off)\n",
    "                    begin = str(token_off[0])\n",
    "                    end = str(token_off[1]) \n",
    "#                     if str(token_off[0]+1) in c_o:\n",
    "#                         if token == c_o[str(token_off[0]+1)][-1][1]:\n",
    "#                             continue\n",
    "#                     if key_value not in c_o:\n",
    "                    if end in c_o_e:\n",
    "                        if token in c_o_e[end]:\n",
    "                            continue\n",
    "                    else:\n",
    "                        c_o_e[end] = set()\n",
    "                    if begin in c_o:\n",
    "                        if token == c_o_s[begin][-1][1]:\n",
    "                            continue\n",
    "                    else:\n",
    "                        c_o_s[begin] = []\n",
    "                    cand.append(token)\n",
    "                    off.append(token_off)\n",
    "                    c_o_s[begin].append((token_off[1],token))\n",
    "                    c_o_e[end].add(token)\n",
    "            cand_name.append(cand)\n",
    "            cand_offset.append(off)\n",
    "            cand_with_off.append(c_o_s)\n",
    "            cand_with_offend.append(c_o_e)\n",
    "#         print(cand_with_offend[0])\n",
    "        return cand_name,cand_offset,cand_with_off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_x = [i['text'] for i in dev_data]\n",
    "print(dev_x[0])\n",
    "en = []\n",
    "en_off = []\n",
    "for i in dev_data:\n",
    "    tmp = []\n",
    "    _ = []\n",
    "    for j in i['mention_data']:\n",
    "        tmp.append(j[0])\n",
    "        _.append((str(j[1]),j[0]))\n",
    "    en.append(tmp)\n",
    "    en_off.append(_)\n",
    "print(en[0])\n",
    "print(en_off[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = ngram_search(dev_x,kb_data.kb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(ns.candidates[0])\n",
    "print(ns.cand_name[0])\n",
    "print(ns.cand_off[0])\n",
    "print(ns.cand_with_off[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extratools.mathtools import safediv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(safediv(1,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cand(cand_name,ground_truth):\n",
    "    recall = []\n",
    "    precision = []\n",
    "    f1 = []\n",
    "    error = []\n",
    "    for i in tqdm(range(len(ground_truth))):\n",
    "        tp = 0\n",
    "        for j in ground_truth[i]:\n",
    "            if j in cand_name[i]:\n",
    "                tp += 1\n",
    "        r = tp/len(ground_truth[i])\n",
    "        p = tp/len(cand_name[i])\n",
    "        f = safediv(2*r*p,r+p)\n",
    "        recall.append(r)\n",
    "        precision.append(p)\n",
    "        if  f > 1 or np.isnan(f):\n",
    "            error.append(i)\n",
    "            f1.append(0)\n",
    "        else:\n",
    "            f1.append(f)\n",
    "    av_recl = sum(recall)/len(recall)\n",
    "    av_pre = sum(precision)/len(precision)\n",
    "    av_f1 = sum(f1)/len(f1)\n",
    "    print('average recall: {}'.format(av_recl))\n",
    "    print('average precision: {}'.format(av_pre))\n",
    "    print('average f1: {}'.format(av_f1))\n",
    "    print('error number: {}'.format(len(error)))\n",
    "    print('total number: {}'.format(len(cand_name)))\n",
    "    print('error rate: {}'.format(len(error)/len(cand_name)))\n",
    "    return recall, precision,f1,error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r1,p1,f11,error = evaluate_cand(ns.cand_name,en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in error:\n",
    "    print(dev_data[i])\n",
    "    print(en[i])\n",
    "    print(ns.cand_name[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = [(1,2),(2,2)]\n",
    "print(a[:][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ajust_para(s,e):\n",
    "    for i in range(1,10):\n",
    "        p = i/10\n",
    "        print('similarity',s+p,'e',e-p)\n",
    "        if s+p <= 0.9:\n",
    "            ns = ngram_search(dev_x,kb_data.kb,similarity=s+p,e = e-p)\n",
    "            evaluate_cand(ns.cand_name,en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ajust_para(0.2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ajust_para(s,e):\n",
    "    for i in range(0,5):\n",
    "        p = i*0.05\n",
    "        print('similarity',s+p,'e',e-p)\n",
    "        if s+p <= 0.9:\n",
    "            ns = ngram_search(dev_x,kb_data.kb,similarity=s+p,e = e-p)\n",
    "            evaluate_cand(ns.cand_name,en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ajust_para(0.6,0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cand_off(cand_with_off,ground_truth,cand_name):\n",
    "    recall = []\n",
    "    precision = []\n",
    "    f1 = []\n",
    "    error = []\n",
    "    error_off = []\n",
    "#     pre1 = []\n",
    "    for i in tqdm(range(len(ground_truth))):\n",
    "        tp = 0\n",
    "        for j in ground_truth[i]:\n",
    "#             print(j)\n",
    "#             print(cand_with_off[i])\n",
    "            if j[0] in cand_with_off[i]:\n",
    "#                 tmp = j[0]\n",
    "                for k in cand_with_off[i][j[0]]:\n",
    "#                     print(j[1])\n",
    "                    if j[1] == k[1]:\n",
    "                        tp += 1\n",
    "                        break;\n",
    "            else:\n",
    "                if j[0] in cand_name[i]:\n",
    "                    error_off.append(i)\n",
    "        r = tp/len(ground_truth[i])\n",
    "#         p1 = tp/len(cand_name[i])\n",
    "        p = tp/len(cand_with_off[i].keys())\n",
    "        f = safediv(2*r*p,r+p)\n",
    "        recall.append(r)\n",
    "        precision.append(p)\n",
    "#         pre1.append(p1)\n",
    "        if  f > 1 or np.isnan(f):\n",
    "            error.append(i)\n",
    "            f1.append(0)\n",
    "        else:\n",
    "            f1.append(f)\n",
    "    av_recl = sum(recall)/len(recall)\n",
    "    av_pre = sum(precision)/len(precision)\n",
    "    av_f1 = sum(f1)/len(f1)\n",
    "#     av_p1 = sum(pre1)/len(pre1)\n",
    "    print('average recall: {}'.format(av_recl))\n",
    "    print('average precision: {}'.format(av_pre))\n",
    "#     print('average p1: {}'.format(av_p1))\n",
    "    print('average f1: {}'.format(av_f1))\n",
    "    print('error number: {}'.format(len(error)))\n",
    "    print('total number: {}'.format(len(cand_name)))\n",
    "    print('error rate: {}'.format(len(error)/len(cand_name)))\n",
    "    return recall, precision,f1,error,error_off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ns = ngram_search(dev_x,kb_data.kb,similarity=0.5,e = 0.6)\n",
    "r,p,f1,er,er_o = evaluate_cand_off(ns.cand_with_off,en_off,ns.cand_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in er:\n",
    "    print(dev_data[i])\n",
    "    print(en[i])\n",
    "    print(ns.cand_name[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in er_o:\n",
    "    print(dev_data[i])\n",
    "    print(en_o[i])\n",
    "    print(en_off[i])\n",
    "#     print(ns.cand_name[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ajust_para(s,e):\n",
    "    for i in range(1,10):\n",
    "        p = i/10\n",
    "        print('similarity',s+p,'e',e-p)\n",
    "        if s+p <= 0.9:\n",
    "            ns = ngram_search(dev_x,kb_data.kb,similarity=s+p,e = e-p)\n",
    "            evaluate_cand_off(ns.cand_with_off,en_off,ns.cand_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ajust_para(0.2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,10):\n",
    "    p = i/10\n",
    "    s = 0.5\n",
    "    e = 1.2\n",
    "    print('similarity',s,'e',e-p)\n",
    "#     if s+p <= 0.9:\n",
    "    ns = ngram_search(dev_x,kb_data.kb,similarity=s,e = e-p)\n",
    "    evaluate_cand_off(ns.cand_with_off,en_off,ns.cand_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,10):\n",
    "    p = i/10\n",
    "    s = 0.5\n",
    "    e = 0.8\n",
    "    print('similarity',s,'e',e-p)\n",
    "#     if s+p <= 0.9:\n",
    "    ns = ngram_search(dev_x,kb_data.kb,similarity=s,e = e-p)\n",
    "    evaluate_cand_off(ns.cand_with_off,en_off,ns.cand_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = [i['text'] for i in train_data]\n",
    "print(train_x[0])\n",
    "train_en = []\n",
    "for i in train_data:\n",
    "    tmp = []\n",
    "    for j in i['mention_data']:\n",
    "        tmp.append(j[0])\n",
    "    train_en.append(tmp)\n",
    "print(train_en[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
